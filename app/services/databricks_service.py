import httpx
import json
import logging
from typing import AsyncGenerator, List
from app.api.v1.schemas.chat import ChatMessage
from app.core.config import settings
from app.services.rag_service import buscar_contexto
from app.services.web_search_service import web_search, deve_pesquisar_web
from llama_cpp import Llama

logger = logging.getLogger(__name__)

class DatabricksService:
    """Servi√ßo para interagir com a API do Databricks e realizar opera√ß√µes relacionadas ao modelo de linguagem e classifica√ß√£o de perguntas m√©dicas."""
    def __init__(self) -> None:
        """Inicializa o cliente HTTP para comunica√ß√£o com o Databricks e tenta carregar um modelo local de Llama para classifica√ß√£o de perguntas m√©dicas, caso esteja dispon√≠vel."""
        self.client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {settings.databricks_token}",
                "Content-Type": "application/json"
            },
            timeout=300.0,
            verify=not settings.debug
        )
        self.endpoint_url = settings.databricks_url
        self.guardrail_llm = None
        try:
            """Carrega o modelo Guardrail Llama-3 localmente para classifica√ß√£o de perguntas m√©dicas, evitando custos de token do Databricks para essa tarefa. O modelo √© otimizado para rodar na CPU, garantindo acessibilidade mesmo sem GPU dedicada."""
            self.guardrail_llm = Llama.from_pretrained(
                repo_id="bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
                filename="Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf",
                n_ctx=1024,
                n_threads=4,
                verbose=False
            )
            logger.info("Guardrail Llama-3 carregado com sucesso na CPU.")
        except Exception as e:
            logger.error(f"Erro ao carregar o modelo Guardrail Llama-3: {e}")
            self.guardrail_llm = None

    async def is_pergunta_medica(self, question: str) -> bool:
        """Verifica escopo localmente na CPU sem gastar tokens do Databricks"""
        if not self.guardrail_llm:
            logger.warning("Guardrail Llama-3 n√£o dispon√≠vel, assumindo que a pergunta √© m√©dica.")
            return True
        
        prompt = (
            f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
            f"Responda apenas SIM ou N√ÉO.<|eot_id|>"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"A pergunta '{question}' √© sobre sa√∫de, medicina ou biologia humana?<|eot_id|>"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
        )

        try:
            """Executa o modelo Guardrail Llama-3 para classificar a pergunta, esperando uma resposta clara de SIM ou N√ÉO. A resposta √© processada para determinar se a pergunta √© m√©dica ou n√£o, e o resultado √© logado para monitoramento e an√°lise futura."""
            output = self.guardrail_llm(prompt, max_tokens=5, stop=["<|eot_id|>"], temperature=0.0)
            resposta = output["choices"][0]["text"].strip().upper()

            is_medical = resposta == "SIM"
            logger.info(f"Guardrail: '{question}' -> {resposta} (M√©dica: {is_medical})")
            return is_medical
        except Exception as e:
            logger.error(f"Erro ao classificar a pergunta com Guardrail Llama-3: {e}")
            return True 

    async def chat_stream(self, question: str, history: List[ChatMessage]) -> AsyncGenerator[str, None]:
        """Gera uma resposta em streaming do Databricks, integrando contexto RAG e resultados de busca web quando aplic√°vel."""
        if not await self.is_pergunta_medica(question):
            yield "Pe√ßo desculpa, mas como MedAi, s√≥ posso responder a quest√µes relacionadas com sa√∫de e medicina. Como posso ajudar com o seu bem-estar hoje?"
            return

        contexto_rag = ""
        contexto_web = ""

        try:
            """Busca contexto relevante usando RAG para a pergunta, o que pode incluir informa√ß√µes de documentos locais ou bases de conhecimento pr√©-indexadas."""
            contexto_rag = buscar_contexto(question)
        except Exception as e:
            logger.error(f"Erro ao buscar contexto RAG: {e}")

        try:
            """Pode ser necess√°rio realizar uma busca na web para obter informa√ß√µes atualizadas ou complementares, especialmente se o contexto local for insuficiente. A decis√£o de buscar na web √© baseada em uma fun√ß√£o que avalia a pergunta e o contexto dispon√≠vel."""
            if deve_pesquisar_web(question):
                logger.info(f"üß† Roteador decidiu buscar na web para: {question}")
                contexto_web = web_search(question)
        except Exception as e:
            logger.error(f"Erro na busca web: {e}")

        system_prompt = (
            "Voc√™ √© o MedAi, um assistente m√©dico inteligente de uma apresenta√ß√£o curta. Voc√™ fala com medicos registrados no CRM n√£o pacientes."
            "Sua tarefa √© fornecer informa√ß√µes baseadas em evid√™ncias.\n\n"
            "sempre me de 5 poss√≠veis diagn√≥sticos ou tratamentos relacionados √† pergunta, mesmo que sejam apenas possibilidades remotas e exames complementares para investiga√ß√£o, e explique o porqu√™ de cada um deles ser relevante para a pergunta. Se poss√≠vel, inclua refer√™ncias bibliogr√°ficas confi√°veis para cada diagn√≥stico ou tratamento sugerido. "
        )

        if contexto_rag:
            system_prompt += f"--- CONTEXTO DOS SEUS DOCUMENTOS ---\n{contexto_rag}\n\n"

        if contexto_web:
            system_prompt += f"--- CONTEXTO ATUALIZADO DA WEB ---\n{contexto_web}\n\n"

        system_prompt += (
            "Importante: Se as informa√ß√µes acima forem conflitantes, priorize os documentos locais. "
            "Sempre cite a fonte e o n√∫mero da p√°gina (ex: Fonte X, p√°g. Y) imediatamente ap√≥s a informa√ß√£o extra√≠da dos documentos locais."
        )

        messages_payload = [{"role": "system", "content": system_prompt}]

        for msg in history:
            messages_payload.append({"role": msg.role, "content": msg.content})
        
        messages_payload.append({"role": "user", "content": question})

        payload = {
            "messages": messages_payload,
            "max_tokens": settings.max_tokens or 1024,
            "temperature": 0.2,
            "stream": True,
        }

        async with self.client.stream("POST", self.endpoint_url, json=payload) as response:
            """Processa a resposta em streaming do Databricks, extraindo e yieldando o conte√∫do √† medida que chega."""
            if response.status_code != 200:
                error = await response.aread()
                raise ValueError(f"Databricks {response.status_code}: {error.decode()}")

            async for line in response.aiter_lines():
                """Cada linha do stream √© esperada no formato 'data: {json}', onde o JSON cont√©m o conte√∫do gerado."""
                stripped = line.strip()
                if stripped.startswith("data: "):
                    data = stripped[6:]
                    if data and data != "[DONE]":
                        try:
                            """Tenta decodificar o JSON da linha para extrair o conte√∫do gerado. O conte√∫do √© esperado no campo 'choices[0].delta.content'. Se o JSON estiver malformado ou n√£o contiver os campos esperados, a linha √© ignorada para evitar interrup√ß√µes no stream."""
                            json_data = json.loads(data)
                            content = json_data['choices'][0]['delta'].get('content', '')
                            if content:
                                yield content
                        except (json.JSONDecodeError, KeyError):
                            continue