import httpx
import json
import logging
from typing import AsyncGenerator, List
from app.api.v1.schemas.chat import ChatMessage
from app.core.config import settings
from app.services.rag_service import buscar_contexto
from app.services.web_search_service import web_search, deve_pesquisar_web
from llama_cpp import Llama

logger = logging.getLogger(__name__)

class DatabricksService:
    """Orquestra guardrail local, contexto (RAG/web) e streaming de respostas via Databricks.

    Attributes:
        client (httpx.AsyncClient): cliente HTTP ass√≠ncrono configurado para o endpoint Databricks.
        endpoint_url (str): URL do endpoint de infer√™ncia.
        guardrail_llm (Optional[Llama]): modelo local usado para classifica√ß√£o de escopo; pode ser None.
    """
    def __init__(self) -> None:
        """Inicializa o servi√ßo.

        Configura o cliente HTTP ass√≠ncrono e tenta carregar um modelo local `Llama`
        usado como guardrail para classifica√ß√£o de perguntas m√©dicas.

        Notes:
            O carregamento do modelo √© 'best-effort' ‚Äî em caso de falha `guardrail_llm`
            ser√° `None` e o servi√ßo continuar√° funcional.
        """
        self.client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {settings.databricks_token}",
                "Content-Type": "application/json"
            },
            timeout=300.0,
            verify=not settings.debug
        )
        self.endpoint_url = settings.databricks_url
        self.guardrail_llm = None
        try:
            # Carrega o modelo Guardrail Llama-3 localmente para classifica√ß√£o de
            # perguntas m√©dicas (best-effort). Em caso de falha, `guardrail_llm`
            # permanecer√° `None`.
            self.guardrail_llm = Llama.from_pretrained(
                repo_id="bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
                filename="Meta-Llama-3.1-8B-Instruct-IQ2_M.gguf",
                n_ctx=1024,
                n_threads=4,
                verbose=False
            )
            logger.info("Guardrail Llama-3 carregado com sucesso na CPU.")
        except Exception as e:
            logger.error(f"Erro ao carregar o modelo Guardrail Llama-3: {e}")
            self.guardrail_llm = None

    async def is_pergunta_medica(self, question: str) -> bool:
        """Classifica se uma pergunta √© de escopo m√©dico.

        Args:
            question (str): pergunta do usu√°rio.

        Returns:
            bool: True se a pergunta for classificada como m√©dica ou se houver
            falha/indisponibilidade do guardrail (fallback permissivo).
        """
        if not self.guardrail_llm:
            logger.warning("Guardrail Llama-3 n√£o dispon√≠vel, assumindo que a pergunta √© m√©dica.")
            return True
        
        prompt = (
            f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
            f"Responda apenas SIM ou N√ÉO.<|eot_id|>"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"A pergunta '{question}' √© sobre sa√∫de, medicina ou biologia humana?<|eot_id|>"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
        )

        try:
            # Executa o modelo guardrail e interpreta resposta 'SIM'/'N√ÉO'.
            output = self.guardrail_llm(prompt, max_tokens=5, stop=["<|eot_id|>"], temperature=0.0)
            resposta = output["choices"][0]["text"].strip().upper()

            is_medical = resposta == "SIM"
            logger.info(f"Guardrail: '{question}' -> {resposta} (M√©dica: {is_medical})")
            return is_medical
        except Exception as e:
            logger.error(f"Erro ao classificar a pergunta com Guardrail Llama-3: {e}")
            return True 

    async def chat_stream(self, question: str, history: List[ChatMessage]) -> AsyncGenerator[str, None]:
        """Gera resposta em streaming a partir do endpoint Databricks.

        Args:
            question (str): pergunta do usu√°rio.
            history (List[ChatMessage]): hist√≥rico de mensagens para contexto.

        Yields:
            str: chunks de texto extra√≠dos do campo `choices[0].delta.content`.

        Raises:
            ValueError: se o endpoint retornar status HTTP != 200.
        """
        if not await self.is_pergunta_medica(question):
            yield "Pe√ßo desculpa, mas como MedAi, s√≥ posso responder a quest√µes relacionadas com sa√∫de e medicina. Como posso ajudar com o seu bem-estar hoje?"
            return

        contexto_rag = ""
        contexto_web = ""

        try:
            # Obt√©m contexto RAG (se dispon√≠vel) para enriquecer o prompt.
            contexto_rag = buscar_contexto(question)
        except Exception as e:
            logger.error(f"Erro ao buscar contexto RAG: {e}")

        try:
            # Quando necess√°rio, realiza busca na web para complementar o contexto.
            if deve_pesquisar_web(question):
                logger.info(f"üß† Roteador decidiu buscar na web para: {question}")
                contexto_web = web_search(question)
        except Exception as e:
            logger.error(f"Erro na busca web: {e}")

        system_prompt = (
            "Voc√™ √© o MedAi, um assistente m√©dico inteligente de uma apresenta√ß√£o curta. Voc√™ fala com medicos registrados no CRM n√£o pacientes."
            "Sua tarefa √© fornecer informa√ß√µes baseadas em evid√™ncias.\n\n"
            "sempre me de 5 poss√≠veis diagn√≥sticos ou tratamentos relacionados √† pergunta, mesmo que sejam apenas possibilidades remotas e exames complementares para investiga√ß√£o, e explique o porqu√™ de cada um deles ser relevante para a pergunta. Se poss√≠vel, inclua refer√™ncias bibliogr√°ficas confi√°veis para cada diagn√≥stico ou tratamento sugerido. "
        )

        if contexto_rag:
            system_prompt += f"--- CONTEXTO DOS SEUS DOCUMENTOS ---\n{contexto_rag}\n\n"

        if contexto_web:
            system_prompt += f"--- CONTEXTO ATUALIZADO DA WEB ---\n{contexto_web}\n\n"

        system_prompt += (
            "Importante: Se as informa√ß√µes acima forem conflitantes, priorize os documentos locais. "
            "Sempre cite a fonte e o n√∫mero da p√°gina (ex: Fonte X, p√°g. Y) imediatamente ap√≥s a informa√ß√£o extra√≠da dos documentos locais."
        )

        messages_payload = [{"role": "system", "content": system_prompt}]

        for msg in history:
            messages_payload.append({"role": msg.role, "content": msg.content})
        
        messages_payload.append({"role": "user", "content": question})

        payload = {
            "messages": messages_payload,
            "max_tokens": settings.max_tokens or 1024,
            "temperature": 0.2,
            "stream": True,
        }

        async with self.client.stream("POST", self.endpoint_url, json=payload) as response:
            # Processa a resposta em streaming do Databricks.
            if response.status_code != 200:
                error = await response.aread()
                raise ValueError(f"Databricks {response.status_code}: {error.decode()}")

            async for line in response.aiter_lines():
                # Cada linha do stream √© esperada no formato 'data: {json}'.
                stripped = line.strip()
                if stripped.startswith("data: "):
                    data = stripped[6:]
                    if data and data != "[DONE]":
                        try:
                            # Extrai `delta.content` do JSON da linha (se existir).
                            json_data = json.loads(data)
                            content = json_data['choices'][0]['delta'].get('content', '')
                            if content:
                                yield content
                        except (json.JSONDecodeError, KeyError):
                            continue